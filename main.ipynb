{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical differentiation of g1 = 0.7499999998827888\n",
      "Backpropagation of g1 = 0.75\n",
      "---------------------------------------------------------------------------\n",
      "Numerical differentiation of g2 = 631.0896884542672\n",
      "Backpropagation of g2 = 631.0896884628216\n",
      "---------------------------------------------------------------------------\n",
      "Numerical differentiation of h1 = [  0.          -4.           8.          48.         127.99999999]\n",
      "Backpropagation of h1 = [  0.  -4.   8.  48. 128.]\n",
      "---------------------------------------------------------------------------\n",
      "Numerical differentiation of h2 = [ 0.00000000e+00 -1.22070312e+01  0.00000000e+00 ...  1.26123047e+05\n",
      "  1.26147461e+05  1.26147461e+05]\n",
      "Numerical differentiation time = 4.192568063735962\n",
      "Backpropagation of h2 = [ 0.00000000e+00 -6.00000000e+00 -3.97939428e+00 ...  1.26125947e+05\n",
      "  1.26141089e+05  1.26156231e+05]\n",
      "Backpropagation time = 0.0007848739624023438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# a function to create a unique increasing ID\n",
    "# note that this is just a quick-and-easy way to create a global order\n",
    "# it's not the only way to do it\n",
    "global_order_counter = 0\n",
    "\n",
    "\n",
    "def get_next_order():\n",
    "    global global_order_counter\n",
    "    rv = global_order_counter\n",
    "    global_order_counter = global_order_counter + 1\n",
    "    return rv\n",
    "\n",
    "# a helper function to convert constants into BackproppableArray objects\n",
    "\n",
    "\n",
    "def to_ba(x):\n",
    "    if isinstance(x, BackproppableArray):\n",
    "        return x\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return BackproppableArray(x)\n",
    "    elif isinstance(x, float):\n",
    "        return BackproppableArray(np.array(x))\n",
    "    elif isinstance(x, int):\n",
    "        return BackproppableArray(np.array(float(x)))\n",
    "    else:\n",
    "        raise Exception(\"could not convert {} to BackproppableArray\".format(x))\n",
    "\n",
    "# a class for an array that can be \"packpropped-through\"\n",
    "\n",
    "\n",
    "class BackproppableArray(object):\n",
    "    # np_array     numpy array that stores the data for this object\n",
    "    def __init__(self, np_array, dependencies=[]):\n",
    "        super().__init__()\n",
    "        self.data = np_array\n",
    "\n",
    "        # grad holds the gradient, an array of the same shape as data\n",
    "        # before backprop, grad is None\n",
    "        # during backprop before grad_fn is called, grad holds the partially accumulated gradient\n",
    "        # after backprop, grad holds the gradient of the loss (the thing we call backward on)\n",
    "        #     with respect to this array\n",
    "        # if you want to use the same array object to call backward twice, you need to re-initialize\n",
    "        #     grad to zero first\n",
    "        self.grad = None\n",
    "\n",
    "        # an counter that increments monotonically over the course of the application\n",
    "        # we know that arrays with higher order must depend only on arrays with lower order\n",
    "        # we can use this to order the arrays for backpropagation\n",
    "        self.order = get_next_order()\n",
    "\n",
    "        # a list of other BackproppableArray objects on which this array directly depends\n",
    "        # we'll use this later to decide which BackproppableArray objects need to participate in the backward pass\n",
    "        self.dependencies = dependencies\n",
    "\n",
    "    # represents me as a string\n",
    "    def __repr__(self):\n",
    "        return \"({}, type={})\".format(self.data, type(self).__name__)\n",
    "\n",
    "    # returns a list containing this array and ALL the dependencies of this array, not just\n",
    "    #    the direct dependencies listed in self.dependencies\n",
    "    # that is, this list should include this array, the arrays in self.dependencies,\n",
    "    #     plus all the arrays those arrays depend on, plus all the arrays THOSE arrays depend on, et cetera\n",
    "    # the returned list must only include each dependency ONCE\n",
    "    def dfs_dependency(visited,self):\n",
    "        if self not in visited:\n",
    "            visited.add(self)\n",
    "            for nodes in self.dependencies:\n",
    "                #the object in self.data is w\n",
    "                BackproppableArray.dfs_dependency(visited,nodes)\n",
    "    def all_dependencies(self):\n",
    "        # TODO: (1.1) implement some sort of search to get all the dependencies\n",
    "        # initialize queue with self.data\n",
    "        visited = set()\n",
    "        BackproppableArray.dfs_dependency(visited,self)\n",
    "        return list(visited)\n",
    "    \n",
    "\n",
    "    # compute gradients of this array with respect to everything it depends on\n",
    "    def backward(self):\n",
    "        # can only take the gradient of a scalar\n",
    "        assert(self.data.size == 1)\n",
    "\n",
    "        # depth-first search to find all dependencies of this array\n",
    "        all_my_dependencies = self.all_dependencies()\n",
    "\n",
    "        # TODO: (1.2) implement the backward pass to compute the gradients\n",
    "        #   this should do the following\n",
    "        #   (1) sort the found dependencies so that the ones computed last go FIRST\n",
    "        sorted_dependencies = sorted(all_my_dependencies, key=lambda x: x.order, reverse=True)\n",
    "        #   (2) initialize and zero out all the gradient accumulators (.grad) for all the dependencies\n",
    "            #set .grad to zeros\n",
    "        for node in sorted_dependencies:\n",
    "            node.grad = np.zeros(node.data.shape)\n",
    "        #   (3) set the gradient accumulator of this array to 1, as an initial condition\n",
    "#         print()\n",
    "        self.grad = np.ones(self.data.shape)\n",
    "        #           since the gradient of a number with respect to itself is 1\n",
    "        #   (4) call the backward function for all the dependencies in the sorted reverse order\n",
    "        for x in sorted_dependencies:\n",
    "            x.grad_fn()\n",
    "\n",
    "    # function that is called to process a single step of backprop for this array\n",
    "    # when called, it must be the case that self.grad contains the gradient of the loss (the\n",
    "    #     thing we are differentating) with respect to this array\n",
    "    # this function should update the .grad field of its dependencies\n",
    "    #\n",
    "    # this should just say \"pass\" for the parent class\n",
    "    #\n",
    "    # child classes override this\n",
    "    def grad_fn(self):\n",
    "        pass\n",
    "\n",
    "    # operator overloading\n",
    "    def __add__(self, other):\n",
    "        return BA_Add(self, to_ba(other))\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return BA_Sub(self, to_ba(other))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return BA_Mul(self, to_ba(other))\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return BA_Div(self, to_ba(other))\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return BA_Add(to_ba(other), self)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return BA_Sub(to_ba(other), self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return BA_Mul(to_ba(other), self)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return BA_Div(to_ba(other), self)\n",
    "\n",
    "    # TODO (2.2) Add operator overloading for matrix multiplication\n",
    "    def __matmul__(self,other):\n",
    "        return BA_MatMul(self,to_ba(other))\n",
    "    def __rmatmul__(self,other):\n",
    "        return BA_MatMul(to_ba(other),self)\n",
    "\n",
    "    def sum(self, axis=None, keepdims=True):\n",
    "        return BA_Sum(self, axis)\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        return BA_Reshape(self, shape)\n",
    "\n",
    "    def transpose(self, axes=None):\n",
    "        if axes is None:\n",
    "            axes = range(self.data.ndim)[::-1]\n",
    "        return BA_Transpose(self, axes)\n",
    "\n",
    "# TODO: implement any helper functions you'll need to backprop through vectors\n",
    "def broadcasting(grad,x):\n",
    "    #Don't need to change the shape of self.x.grad.shape\n",
    "    #grad always have more dimension that x\n",
    "    # reshape self.grad to x.grad\n",
    "    # change the shape of the gradient\n",
    "    shape = x.shape\n",
    "    while len(shape)<len(grad.shape):\n",
    "        shape = (1,)+shape\n",
    "#     print(shape)\n",
    "    differ_idx = []\n",
    "    for i in range(len(grad.shape)):\n",
    "        if shape[i]==1 and grad.shape[i]!=1:\n",
    "            differ_idx.append(i)\n",
    "    if len(differ_idx)==0:\n",
    "        return grad\n",
    "    else:\n",
    "#         for idx in differ_idx:\n",
    "        grad = grad.sum(axis =tuple(differ_idx))\n",
    "        return grad.reshape(x.shape)\n",
    "    \n",
    "# a class for an array that's the result of an addition operation\n",
    "\n",
    "\n",
    "class BA_Add(BackproppableArray):\n",
    "    # x + y\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(x.data + y.data, [x, y])\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (2.3) improve grad fn for Add\n",
    "#       \n",
    "        self.x.grad += broadcasting(self.grad,self.x.grad)\n",
    "        self.y.grad +=broadcasting(self.grad,self.y.grad)\n",
    "        return \n",
    "        #1.3\n",
    "        self.x.grad += self.grad\n",
    "        self.y.grad += self.grad\n",
    "# a class for an array that's the result of a subtraction operation\n",
    "\n",
    "\n",
    "class BA_Sub(BackproppableArray):\n",
    "    # x + y\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(x.data - y.data, [x, y])\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (1.3, 2.3) implement grad fn for Sub\n",
    "        #2.3\n",
    "        self.x.grad += broadcasting(self.grad,self.x.grad)\n",
    "        self.y.grad +=broadcasting(self.grad,self.y.grad)\n",
    "        return\n",
    "        #1.3\n",
    "        self.x.grad += self.grad\n",
    "        self.y.grad += self.grad\n",
    "#         pass\n",
    "\n",
    "# a class for an array that's the result of a multiplication operation\n",
    "\n",
    "\n",
    "class BA_Mul(BackproppableArray):\n",
    "    # x * y\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(x.data * y.data, [x, y])\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (1.3, 2.3) implement grad fn for Mul\n",
    "        #2.3\n",
    "        self.x.grad += self.y.data*broadcasting(self.grad,self.x.grad)\n",
    "        self.y.grad += self.x.data*broadcasting(self.grad,self.y.grad)\n",
    "        return\n",
    "        #1.3\n",
    "        self.x.grad += self.y.data*self.grad\n",
    "        self.y.grad += self.x.data*self.grad\n",
    "        \n",
    "#         pass\n",
    "\n",
    "# a class for an array that's the result of a division operation\n",
    "\n",
    "\n",
    "class BA_Div(BackproppableArray):\n",
    "    # x / y\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(x.data / y.data, [x, y])\n",
    "        self.x = x  \n",
    "        self.y = y\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (1.3, 2.3) implement grad fn for Div\n",
    "#         if self.grad.shape != self.x.grad.shape:\n",
    "         #2.3\n",
    "        self.x.grad += broadcasting(self.grad,self.x.grad.shape)/self.y.data\n",
    "        self.y.grad -= self.x.data*broadcasting(self.grad,self.y.grad.shape)/(self.y.data*self.y.data)\n",
    "        return\n",
    "        #1.3\n",
    "        self.x.grad += self.grad/self.y.data\n",
    "        self.y.grad-=self.x.data*self.grad/(self.y.data*self.y.data)\n",
    "\n",
    "# a class for an array that's the result of a matrix multiplication operation\n",
    "class BA_MatMul(BackproppableArray):\n",
    "    # x @ y\n",
    "    def __init__(self, x, y):\n",
    "        # we only support multiplication of matrices, i.e. arrays with shape of length 2\n",
    "        assert(len(x.data.shape) == 2)\n",
    "        assert(len(y.data.shape) == 2)\n",
    "        super().__init__(x.data @ y.data, [x, y])\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (2.1) implement grad fn for MatMul\n",
    "        self.x.grad += self.grad@(self.y.data.T)\n",
    "        self.y.grad += self.x.data.T@self.grad\n",
    "#         print('MatMul Gradient x: ',self.x.grad)\n",
    "#         print('MatMul Gradient y: ',self.y.grad)\n",
    "#         pass\n",
    "\n",
    "\n",
    "# a class for an array that's the result of an exponential operation\n",
    "class BA_Exp(BackproppableArray):\n",
    "    # exp(x)\n",
    "    def __init__(self, x):\n",
    "        super().__init__(np.exp(x.data), [x])\n",
    "        self.x = x\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (1.3) implement grad fn for Exp\n",
    "        self.x.grad += exp(self.x)*self.grad\n",
    "#         pass\n",
    "\n",
    "\n",
    "def exp(x):\n",
    "    if isinstance(x, BackproppableArray):\n",
    "        return BA_Exp(x)\n",
    "    else:\n",
    "        return np.exp(x)\n",
    "\n",
    "# a class for an array that's the result of an logarithm operation\n",
    "\n",
    "\n",
    "class BA_Log(BackproppableArray):\n",
    "    # log(x)\n",
    "    def __init__(self, x):\n",
    "        super().__init__(np.log(x.data), [x])\n",
    "        self.x = x\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (1.3) implement grad fn for Log\n",
    "        self.x.grad += 1/self.x.data*self.grad\n",
    "#         pass\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    if isinstance(x, BackproppableArray):\n",
    "        return BA_Log(x)\n",
    "    else:\n",
    "        return np.log(x)\n",
    "\n",
    "# TODO: Add your own function\n",
    "# END TODO\n",
    "\n",
    "# a class for an array that's the result of a sum operation\n",
    "\n",
    "\n",
    "class BA_Sum(BackproppableArray):\n",
    "    # x.sum(axis, keepdims=True)\n",
    "    def __init__(self, x, axis):\n",
    "        super().__init__(x.data.sum(axis, keepdims=True), [x])\n",
    "        self.x = x\n",
    "        self.axis = axis\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (2.1) implement grad fn for Sum\n",
    "        self.x.grad += self.grad\n",
    "\n",
    "# a class for an array that's the result of a reshape operation\n",
    "\n",
    "\n",
    "class BA_Reshape(BackproppableArray):\n",
    "    # x.reshape(shape)\n",
    "    def __init__(self, x, shape):\n",
    "        super().__init__(x.data.reshape(shape), [x])\n",
    "        self.x = x\n",
    "        self.shape = shape\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (2.1) implement grad fn for Reshape\n",
    "        self.x.grad += self.grad.reshape(self.x.grad.shape)\n",
    "#         print(self.x.grad)\n",
    "\n",
    "# a class for an array that's the result of a transpose operation\n",
    "\n",
    "\n",
    "class BA_Transpose(BackproppableArray):\n",
    "    # x.transpose(axes)\n",
    "    def __init__(self, x, axes):\n",
    "        super().__init__(x.data.transpose(axes), [x])\n",
    "        self.x = x\n",
    "        self.axes = axes\n",
    "\n",
    "    def grad_fn(self):\n",
    "        # TODO: (2.1) implement grad fn for Transpose\n",
    "        self.x.grad += self.grad.transpose(self.axes)\n",
    "#         pass\n",
    "\n",
    "\n",
    "# numerical derivative of scalar function f at x, using tolerance eps\n",
    "def numerical_diff(f, x, eps=1e-5):\n",
    "    return (f(x + eps) - f(x - eps))/(2*eps)\n",
    "\n",
    "\n",
    "def numerical_grad(f, x, eps=1e-5):\n",
    "    # TODO: (2.5) implement numerical gradient function\n",
    "    #       this should compute the gradient by applying something like\n",
    "    #       numerical_diff independently for each entry of the input x\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i] = 1\n",
    "        grad[i]=(f(x+eps*e_i)-f(x-eps*e_i))/(2*eps)\n",
    "    return grad    \n",
    "\n",
    "# automatic derivative of scalar function f at x, using backprop\n",
    "\n",
    "\n",
    "def backprop_diff(f, x):\n",
    "    ba_x = to_ba(x)\n",
    "    fx = f(ba_x)\n",
    "    fx.backward()\n",
    "    return ba_x.grad\n",
    "\n",
    "\n",
    "# class to store test functions\n",
    "class TestFxs(object):\n",
    "    # scalar-to-scalar tests\n",
    "    @staticmethod\n",
    "    def f1(x):\n",
    "        return x * 2 + 3\n",
    "\n",
    "    @staticmethod\n",
    "    def df1dx(x):\n",
    "        # TODO (1.4) implement symbolic derivative of f1\n",
    "        return 2\n",
    "\n",
    "    @staticmethod\n",
    "    def f2(x):\n",
    "        return x * x\n",
    "\n",
    "    @staticmethod\n",
    "    def df2dx(x):\n",
    "        # TODO (1.4) implement symbolic derivative of f2\n",
    "        return 2*x\n",
    "#         pass\n",
    "\n",
    "    @staticmethod\n",
    "    def f3(x):\n",
    "        u = (x - 2.0)\n",
    "        return u / (u*u + 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def df3dx(x):\n",
    "        # TODO (1.4) implement symbolic derivative of f3\n",
    "        return (-x**2+4*x-3)/(x**2-4*x+5)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def f4(x):\n",
    "        return log(exp(x*x / 8 - 3*x + 5) + x)\n",
    "\n",
    "    # scalar-to-scalar tests that use vectors in the middle\n",
    "    @staticmethod\n",
    "    def g1(x):\n",
    "        a = np.ones(3, dtype=\"float64\")\n",
    "        ax = x + a\n",
    "        return log((ax*ax)).sum().reshape(())\n",
    "\n",
    "    @staticmethod\n",
    "    def g2(x):\n",
    "        a = np.ones((4, 5), dtype=\"float64\")\n",
    "        b = np.arange(20, dtype=\"float64\")\n",
    "        ax = x - a\n",
    "        bx = log((x + b)*(x + b)).reshape((4, 5)).transpose()\n",
    "        y = bx @ ax\n",
    "        return y.sum()\n",
    "\n",
    "    # vector-to-scalar tests\n",
    "    @staticmethod\n",
    "    def h1(x):  # takes an input of shape (5,)\n",
    "        b = np.arange(5, dtype=\"float64\")\n",
    "        xb = x * b - 4\n",
    "        return (xb * xb).sum().reshape(())\n",
    "\n",
    "    # TODO: Add any other test functions you want to use here\n",
    "    def h2(x):\n",
    "        a =np.arange(10000,dtype=\"float64\")**0.1\n",
    "        xa =x * a  - 4\n",
    "        return (xa*xa).sum().reshape(())\n",
    "    # END TODO\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: Test your code using the provided test functions and your own functions\n",
    "    x = 7\n",
    "\n",
    "#     #Symbolic diff\n",
    "#     print('Symbolic differentation of f1 = {}'.format(TestFxs.df1dx(x)))\n",
    "#     print('Symbolic differentation of f2 = {}'.format(TestFxs.df2dx(x)))\n",
    "#     print('Symbolic differentation of f3 = {}'.format(TestFxs.df3dx(x)))\n",
    "\n",
    "#     #numerical diff\n",
    "#     print('Numerical differentation of f1 = {}'.format(numerical_diff(TestFxs.f1,x)))\n",
    "#     print('Numerical differentation of f2 = {}'.format(numerical_diff(TestFxs.f2,x)))\n",
    "#     print('Numerical differentation of f3 = {}'.format(numerical_diff(TestFxs.f3,x)))\n",
    "#     print('Numerical differentation of f4 = {}'.format(numerical_diff(TestFxs.f4,x)))\n",
    "\n",
    "#     #AD\n",
    "#     print('Backpropagation of f1 = {}'.format(backprop_diff(TestFxs.f1,x)))\n",
    "#     print('Backpropagation of f2 = {}'.format(backprop_diff(TestFxs.f2,x)))\n",
    "#     print('Backpropagation of f3 = {}'.format(backprop_diff(TestFxs.f3,x)))\n",
    "#     print('Backpropagation of f4 = {}'.format(backprop_diff(TestFxs.f4,x)))\n",
    "    \n",
    "    #Test g1 & g2\n",
    "    print('Numerical differentiation of g1 = {}'.format(numerical_diff(TestFxs.g1,x)))\n",
    "    print('Backpropagation of g1 = {}'.format(backprop_diff(TestFxs.g1,x)))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('Numerical differentiation of g2 = {}'.format(numerical_diff(TestFxs.g2,x)))\n",
    "    print('Backpropagation of g2 = {}'.format(backprop_diff(TestFxs.g2,x)))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    #Test h1\n",
    "    y = np.array([1,2,3,4,5])\n",
    "    print('Numerical differentiation of h1 = {}'.format(numerical_grad(TestFxs.h1,y)))\n",
    "    print('Backpropagation of h1 = {}'.format(backprop_diff(TestFxs.h1,y)))\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    #Test h2\n",
    "    z = np.arange(10000)\n",
    "    start1 = time.time()\n",
    "    print('Numerical differentiation of h2 = {}'.format(numerical_grad(TestFxs.h2,z)))\n",
    "    end1 = time.time()\n",
    "    print('Numerical differentiation time = {}'.format(end1-start1) )\n",
    "    start2= time.time()\n",
    "    print('Backpropagation of h2 = {}'.format(backprop_diff(TestFxs.h2,z)))\n",
    "    end2 = time.time()\n",
    "    print('Backpropagation time = {}'.format(end2-start2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.99856e+10,\n",
       "       1.99904e+10, 1.99968e+10])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_grad(TestFxs.h2,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.00000000e+01, 2.64574061e+02, 4.77186550e+02, ...,\n",
       "       1.99860884e+10, 1.99920466e+10, 1.99980060e+10])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backprop_diff(TestFxs.h2,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
